{
	"name": "SparkServing - Deploying a Classifier",
	"properties": {
		"folder": {
			"name": "spark_serving"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "45a46fcd-f970-4cca-8e93-4361e74bef33"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"## Model Deployment with Spark Serving \n",
					"In this example, we try to predict incomes from the *Adult Census* dataset. Then we will use Spark serving to deploy it as a realtime web service. \n",
					"First, we import needed packages:"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import os\n",
					"if os.environ.get(\"AZURE_SERVICE\", None) == \"Microsoft.ProjectArcadia\":\n",
					"    from pyspark.sql import SparkSession\n",
					"    spark = SparkSession.builder.getOrCreate()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": true
				},
				"source": [
					"import sys\n",
					"import numpy as np\n",
					"import pandas as pd\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"Now let's read the data and split it to train and test sets:"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"data = spark.read.parquet(\"wasbs://publicwasb@mmlspark.blob.core.windows.net/AdultCensusIncome.parquet\")\n",
					"data = data.select([\"education\", \"marital-status\", \"hours-per-week\", \"income\"])\n",
					"train, test = data.randomSplit([0.75, 0.25], seed=123)\n",
					"train.limit(10).toPandas()"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"`TrainClassifier` can be used to initialize and fit a model, it wraps SparkML classifiers.\n",
					"You can use `help(synapse.ml.TrainClassifier)` to view the different parameters.\n",
					"\n",
					"Note that it implicitly converts the data into the format expected by the algorithm. More specifically it:\n",
					" tokenizes, hashes strings, one-hot encodes categorical variables, assembles the features into a vector\n",
					"etc.  The parameter `numFeatures` controls the number of hashed features."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": true
				},
				"source": [
					"from synapse.ml.train import TrainClassifier\n",
					"from pyspark.ml.classification import LogisticRegression\n",
					"model = TrainClassifier(model=LogisticRegression(), labelCol=\"income\", numFeatures=256).fit(train)"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"After the model is trained, we score it against the test dataset and view metrics."
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from synapse.ml.train import ComputeModelStatistics, TrainedClassifierModel\n",
					"prediction = model.transform(test)\n",
					"prediction.printSchema()"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"metrics = ComputeModelStatistics().transform(prediction)\n",
					"metrics.limit(10).toPandas()"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"First, we will define the webservice input/output.\n",
					"For more information, you can visit the [documentation for Spark Serving](https://github.com/Microsoft/SynapseML/blob/master/docs/mmlspark-serving.md)"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import *\n",
					"from synapse.ml.io import *\n",
					"import uuid\n",
					"\n",
					"serving_inputs = spark.readStream.server() \\\n",
					"    .address(\"localhost\", 8898, \"my_api\") \\\n",
					"    .option(\"name\", \"my_api\") \\\n",
					"    .load() \\\n",
					"    .parseRequest(\"my_api\", test.schema)\n",
					"\n",
					"serving_outputs = model.transform(serving_inputs) \\\n",
					"  .makeReply(\"prediction\")\n",
					"\n",
					"server = serving_outputs.writeStream \\\n",
					"    .server() \\\n",
					"    .replyTo(\"my_api\") \\\n",
					"    .queryName(\"my_query\") \\\n",
					"    .option(\"checkpointLocation\", \"file:///tmp/checkpoints-{}\".format(uuid.uuid1())) \\\n",
					"    .start()\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"Test the webservice"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import requests\n",
					"data = u'{\"education\":\" 10th\",\"marital-status\":\"Divorced\",\"hours-per-week\":40.0}'\n",
					"r = requests.post(data=data, url=\"http://localhost:8898/my_api\")\n",
					"print(\"Response {}\".format(r.text))"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import requests\n",
					"data = u'{\"education\":\" Masters\",\"marital-status\":\"Married-civ-spouse\",\"hours-per-week\":40.0}'\n",
					"r = requests.post(data=data, url=\"http://localhost:8898/my_api\")\n",
					"print(\"Response {}\".format(r.text))"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": true
				},
				"source": [
					"import time\n",
					"time.sleep(20) # wait for server to finish setting up (just to be safe)\n",
					"server.stop()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": true
				},
				"source": [
					""
				]
			}
		]
	}
}